\chapter{Literature Review}

\todo{This is all very general: perhaps make more specific as more
  details of the project get worked out}

This chapter reviews the background of the project: the traditional
methods of garbage collection, more complicated collectors, methods of
algorithm and software verification, and current work in the
production of verified garbage collectors.

\section{Garbage Collection}

Garbage collection is the process of automatically reclaiming unneeded
memory from a program. In languages with manual memory management,
such as C, a programmer can allocate new memory with \texttt{malloc},
but then must remember to release it with \texttt{free}. If this is
not done, a memory leak occurs, and the program may gradually consume
more and more memory.

In a garbage collected language, however, the programmer can allocate
new memory, but typically cannot (and does not need to) explicitly
deallocate it. Instead, some runtime system determines when blocks of
memory are no longer needed, typically by determining if they can be
reached by following pointers from the ``roots''---the set of
variables in scope---or not.

\subsection{The Traditional Algorithms}

We can divide most garbage collection algorithms into four camps:
reference counting, mark-sweep, copying, and mark-compact. Each is
suited to different use-cases, and the choice of which to use is often
determined by the behaviour of the particular system in which they
will be used.

\subsubsection{Reference Counting}

Reference counting is a very simple scheme where each block contains a
\textit{reference count}, some integer field which keeps track of the
number of references to the block. In the most basic form of the
algorithm, the compiler maintains a write barrier, which increments
and decrements the count immediately as references are created or
destroyed. Furthermore, as soon as the count reaches zero, the block
is deallocated\cite{Collins60}.

This strategy seems good, as it distributes the load of memory
management throughout the computation, however it results in the
problem of unbounded time taken to free a block, as freeing it may
cause other things to be freed, and so on\cite{GarbageCollection}.
This problem can be offset by pushing to a free list, rather than
doing the recursive free, or by deferring the reference counting.

The Deutsch--Bobrow algorithm is an example of this kind, where
reference count adjustments are stored in a transaction file, which is
routinely processed to adjust the state of the entire
system\cite{Deutsch76}.

A fatal flaw, alas, with reference counting is that it cannot reclaim
self-referential structures, even if they are
garbage\cite{McBeth63}. Some systems make use of ``weak references'',
references which do not cause the count to be modified, to offset
this. However, this places a burden on the programmer and so loses
some of the convenience of the system.

\subsubsection{Mark-Sweep}

The mark-sweep method was invented for the early Lisp
systems\cite{McCarthy60}, and it functions by giving every block a
\textit{mark bit}, which is initially unset. Upon running out of
memory, all cells reachable from the roots are marked (this can be
done recursively quite simply), and then all blocks in the heap are
examined: those which are unmarked are freed, and those which are
marked are unmarked, ready for the next collection.

Compared to reference counting, mark-sweep is a very different beast:
it handles cycles easily and there is no overhead on pointer
operations. It does, however, ``stop the world'' in order to perform a
collection\cite{GarbageCollection}. Furthermore, garbage collection
becomes more frequent as heap residency increases.

These problems can be offset somewhat by interleaving the sweeping
with the mutator (the user program, so called because it mutates the
heap). Hughes's lazy sweep algorithm\cite{Hughes82} does this by
performing a fixed amount of sweeping at each allocation, and so the
only long garbage collection pause happens when the heap needs to be
marked.

\subsubsection{Copying}

Copying collectors divide the heap into two \textit{semispaces}, where
allocation is only done in one of them at a time. In garbage
collection, all live block are copied to the other semispace, and the
roles of the semispaces are swapped\cite{Fenichel69}. Unlike
mark-sweep collectors, the pause time of a copying collector is
proportional only to the number of live blocks in the heap, rather
than the size of the entire heap.

Copying collectors have become fairly popular due to the low cost of
allocation and reduction of fragmentation, however it has a cost of a
half of the heap space\cite{GarbageCollection}.

\subsubsection{Mark-Compact}

Heap fragmentation can be a great problem: there may be more than
enough space to allocate something, but not enough contiguous space to
do so. Furthermore, high fragmentation can increase the rate of page
faults and cache misses, reducing the performance of the mutator.

Thus, we come to the mark-compact collectors. These first mark the
live portion of the heap, and then copy marked blocks over garbage
ones, moving everything towards one end of the heap in order to remove
fragmentation. There are three ways of performing this compacting:
blocks can be moved without regard for their original order; blocks
which point to each other can be placed next to each other; and blocks
can simply ``slide'' towards one end of the heap.

Unfortunately, these collectors require multiple traversals over the
heap, and so are potentially even slower than mark-sweep
collectors\cite{GarbageCollection}.

\subsection{Hybrid Collectors}

Sometimes any one of the traditional algorithms is not quite good
enough for a particular problem, and a better collector can be formed
by combining them. For example, reference counting could be combined
with periodic copying, in order to reclaim cyclic structures and
reduce fragmentation. For another example, Immix\cite{Blackburn08} is
a hybrid mark-region (memory is divided into large regions, inside
which allocation occurs) and copying collector.

\subsubsection{Generational Collectors}

Generational garbage collection is a type of hybrid collector: the
heap is divided into a number of \textit{generations}, where each
generation holds progressively older blocks. This is based on
empirical observations of block lifetimes resulting in the
generational hypothesis: young objects die quickly, whereas old
objects stick around\cite{Ungar84}.

Allocation happens in the youngest generation, and when that fills up
a \textit{minor collection} is started. The generation is garbage
collected, and old enough blocks get copied to the next generation. In
the simplest case, all live blocks at the time of a minor collection
get promoted. If the entire heap is full, a \textit{major collection}
is started, which uses some other garbage collection
algorithm\cite{GarbageCollection}.

Generational garbage collection tends to perform well for languages
where old-to-young pointers are rare, such as most functional
languages, and so has become fairly popular amongst implementers of
such languages.

\section{Algorithm Verification}

Algorithm, or software, verification is the process of constructing a
mathematical proof that an algorithm behaves as we expect it to. In
the case of software, we may prove the actual code implements the
specification, in the case of an algorithm, we may be satisfied with
merely proving that it satisfies various properties (such as
termination).

We need verification because testing can never show that there are no
bugs, it can only show that there have been no bugs found yet. A
formal proof provides a much stronger guarantee than can be obtained
by testing or inspection alone.

There are two methods of verification: constructing the algorithm and
then proving things about it, and constructing a proof and then
extracting an algorithm out of it.

\subsection{Verification by Proof}

This is arguably the more difficult method of verification, as it
consists of taking a fairly low-level implementation (often at the
level of code) and a very high-level specification and proving things
about the two.

To begin with, the implementation must be formalised using some logic
such as Hoare logic\cite{Hoare69}, which define the semantics for a
very simple imperative language, or separation logic\cite{Reynolds02},
which is similar to Hoare logic but adds axioms for reasoning about
the stack and heap.

Once formalised, the axioms of the logic can be used to deduce
propositions about the implementation. This can be, to some extent,
automated using proof assistants, but in general this is undecidable.

\subsection{Verification by Extraction}

The other approach is to gradually work downwards from the high-level
specification to the low-level implementation. This is called
extraction, or refinement.

We start out by formalising the specification in some logic, such as Z
or HOL, and then construct a slightly lower-level specification and
prove that it is a correct refinement of the higher-level one. Whilst
it may take many refinement steps to get actual code out of this
process, each proof is between two very similar specifications, and so
may be easier than the verification by proof method. Proof assistants
may be more useful here than in verification by proof, but here the
difficulty lies in finding the appropriate invariant to relate the two
steps.

Furthermore, there are often multiple choices of refinement step
available, and so first few steps can be reused to construct different
algorithms, simply by making different choices at the lower levels.

\subsection{Proof Assistants}

\todo{If I decide to use one, brief summary of what they are, and then
  a slightly more detailed subsubsection about the one I will use}

\section{Verified Garbage Collection}

Finally, we look at two examples of garbage collectors which have been
verified in different ways.

\subsection{Reusable verification of a copying collector}

Myreen\cite{Myreen10} has constructed a verified garbage collector
using the extraction method: correctness was formalised using the HOL4
theorem prover, was refined to produce an algorithm for correct
copying collectors, and was then refined further to the level of
verified machine code.

The collectors produced have been named L1, which is a specification
of garbage collection; L2, which is an implementation of L1 as the
transitive closure of a step relation; L3, which is a deterministic
implementation of L2 with more realistic memory semantics; L4, which
uses actual implementation types; and L5, which is verified machine
code. L5 is constructed by earlier work of Myreen, on constructing a
certifying compiler.

Other than the obvious, the main contribution of Myreen is that his
proof is quite generic, as only the lowest level of refinement (L5)
makes use of a specific programming logic, and could be applied to many
different copying collectors, just by changing the specific refinement
steps taken.

It is possible that his L1 collector could be applied to non-copying
collectors, providing an already formalised notion of correctness
``for free''.

\subsection{Local Reasoning about a Copying Garbage Collector}

Birkedal, Torp-Smith, and Reynolds\cite{Birkedal04} take a very
different approach to Myreen, they take an existing garbage collector
(Cheney's copying collector) and prove it correct, using a variant of
separation logic.

Similarly to Myreen, they define correctness in terms of a heap
isomorphism modulo garbage cells, and in particular, that the
resultant heap has no garbage cells whatsoever.

The motivation of this paper is to be a stepping stone towards
programming logics which incorporate garbage collection, and so being
able to prove the correctness of the combination of a program and a
language runtime, which has thus far not been feasible.

This paper contributes to the field an extension of separation logic
with semantics for more types of assertion over finite sets, which is
used to express the isomorphism property of the algorithm. Needless to
say, this is an essential property of any garbage collector which
moves data around in the heap.

The major difference between these two verified garbage collectors is
that Myreen used extraction whereas Birkedal \textit{et al.} used
proof. Thus, this proof cannot really be generalised to other
collectors (although the definition of correctness), whereas Myreen's
proofs are much more applicable to copying collectors in general.
