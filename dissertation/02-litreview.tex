\chapter{Literature Review}
\label{sec:lit}

In this chapter, I shall review the work that has already been done in
both the fields of \gls{garbage collection} in section
\ref{sec:lit-gc}, and \gls{algorithm verification} in section
\ref{sec:lit-verification}, and then consider in more detail two
different verified \glspl{collector} in section
\ref{sec:lit-verifiedgc}. I shall restrict the review largely to
stop-the-world, non-concurrent, ``non-hybrid'' \glspl{collector}, by
which I mean a \gls{collector} which uses just one method to garbage
collect the \gls{heap}, rather than combining multiple methods. This
is because I shall only be reasoning about such simple
\glspl{collector}, as considering more complex situations would
significantly complicate the work. I shall briefly mention
generational \glspl{collector}, which are arguably hybrid, but shall
not be rigorously proving things about them.

\section{Garbage Collection}
\label{sec:lit-gc}

Garbage collection is the process of automatically reclaiming unneeded
allocated memory from a program. In languages with manual memory
management, such as C, a programmer can allocate new memory with
\texttt{malloc}, but then must remember to release it with
\texttt{free}. If this is not done, a memory leak occurs, and the
program may gradually consume more and more memory\cite{KandR}.

In a garbage collected language, however, the programmer can allocate
new memory, but typically cannot (and does not need to) explicitly
deallocate it. Instead, some runtime system determines when
\glspl{cell} no longer needed, typically by determining if they can be
reached by following pointers from the \glspl{root}---the set of
variables in scope---or not.

\subsection{The Traditional Algorithms}
\label{sec:lit-gc-traditional}

We can divide most garbage collection algorithms into four camps:
\gls{reference counting}, \gls{mark-sweep}, \gls{copying}, and
\gls{mark-compact}. Each is suited to different use-cases, and the
choice of which to use is often determined by the behaviour of the
particular system in which they will be used.

Throughout this section, I shall be using a visual example to aid
understanding. Figure \ref{fig:lit-gc-before} shows a \gls{heap}
before \gls{garbage collection}, and for each \gls{collector}, I shall
show the same \gls{heap} after collection. Green \glspl{cell} are
those pointed to by the \glspl{root}, black are not garbage, red are
garbage, and white are unallocated. A reference count is displayed in
each \gls{cell}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{lit-gc-before}
  \caption{Heap with garbage}
  \label{fig:lit-gc-before}
\end{figure}

\subsubsection{Reference Counting}
\label{sec:lit-gc-traditional-refcount}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{lit-gc-refcount}
  \caption{Heap after reference counting}
  \label{fig:lit-gc-refcount}
\end{figure}

Reference counting is a very simple scheme where each \gls{cell}
contains a \textit{reference count}, some integer field which keeps
track of the number of references to the \gls{cell}. In the most basic
form of the algorithm, the compiler maintains a write barrier, which
increments and decrements the count immediately as references are
created or destroyed. Furthermore, as soon as the count reaches zero,
the \gls{cell} is deallocated\cite{Collins60}.

This strategy seems good, as it distributes the load of memory
management throughout the computation, however it results in the
problem of unbounded time taken to free a \gls{cell}, as freeing it
may cause other things to be freed, and so on\cite{GarbageCollection}.
This problem can be offset by pushing to a \gls{free list}, rather
than doing the recursive free, or by deferring the reference counting.

The Deutsch--Bobrow algorithm is an example of this kind, where
reference count adjustments are stored in a transaction file, which is
routinely processed to adjust the state of the entire
system\cite{Deutsch76}.

A fatal flaw, alas, with reference counting is that it cannot reclaim
self-referential structures, as shown in the example, even if they are
garbage\cite{McBeth63}. Some systems make use of ``\glspl{weak
  reference}'', references which do not cause the count to be
modified, to offset this. However, this places a burden on the
programmer and so loses some of the convenience of the system.

\subsubsection{Mark-Sweep}
\label{sec:lit-gc-traditional-marksweep}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{lit-gc-marksweep}
  \caption{Heap after mark-sweep collection}
  \label{fig:lit-gc-marksweep}
\end{figure}

The mark-sweep method was invented for the early Lisp
systems\cite{McCarthy60}, and it functions by giving every \gls{cell}
a \textit{mark bit}, which is initially unset. Upon running out of
memory, all cells reachable from the roots are marked (this can be
done recursively quite simply), and then all \glspl{cell} in the heap
are examined: those which are unmarked are freed, and those which are
marked are unmarked, ready for the next collection.

Compared to reference counting, mark-sweep is a very different beast:
it handles cycles easily and there is no overhead on pointer
operations. It does, however, ``\gls{stop the world}'' in order to
perform a collection\cite{GarbageCollection}. Furthermore, garbage
collection becomes more frequent as \gls{heap residency} increases,
and the blocking time is proportional to the size of the heap.

These problems can be offset somewhat by interleaving the sweeping
with the \gls{mutator} (the user program, so called because it mutates
the heap). Hughes's lazy sweep algorithm\cite{Hughes82} does this by
performing a fixed amount of sweeping at each allocation, and so the
only long garbage collection pause happens when the heap needs to be
marked.

\subsubsection{Copying}
\label{sec:lit-gc-traditional-copying}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{lit-gc-copying}
  \caption{Heap after copying collection}
  \label{fig:lit-gc-copying}
\end{figure}

Copying collectors divide the heap into two \textit{semispaces}, where
allocation is only done in one of them at a time. In garbage
collection, all live \gls{cell} are copied to the other semispace (in
the example this has been shown by shifting the addresses up by 8),
and the roles of the semispaces are swapped\cite{Fenichel69}. Unlike
mark-sweep collectors, the pause time of a copying collector is
proportional only to the number of live \glspl{cell}.

Copying collectors have become fairly popular due to the low cost of
allocation and reduction of fragmentation, however it has a space cost
of a half of the heap space\cite{GarbageCollection}.

\subsubsection{Mark-Compact}
\label{sec:lit-gc-traditional-markcompact}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{lit-gc-markcompact}
  \caption{Heap after mark-compact collection}
  \label{fig:lit-gc-markcompact}
\end{figure}

Heap \gls{fragmentation} can be a great problem: there may be more
than enough space to allocate something, but not enough contiguous
space to do so. Furthermore, high fragmentation can increase the rate
of page faults and cache misses, reducing the performance of the
mutator.

Thus, we come to the mark-compact collectors. These first mark the
live portion of the heap, and then copy marked \glspl{cell} over
garbage ones, moving everything towards one end of the heap in order
to remove fragmentation. There are three ways of performing this
compacting: \glspl{cell} can be moved without regard for their
original order; \glspl{cell} which point to each other can be placed
next to each other; and \glspl{cell} can simply ``slide'' towards one
end of the heap.

Unfortunately, these collectors require multiple traversals over the
heap, and so are potentially even slower than mark-sweep
collectors\cite{GarbageCollection}.

\subsection{Hybrid Collectors}
\label{sec:lit-gc-hybrid}

Sometimes any one of the traditional algorithms is not quite good
enough for a particular problem, and a better collector can be formed
by combining them. For example, reference counting could be combined
with periodic copying, in order to reclaim cyclic structures and
reduce fragmentation. For another example, Immix\cite{Blackburn08} is
a hybrid mark-region (memory is divided into large regions, inside
which allocation occurs) and copying collector.

\subsubsection{Generational Collectors}
\label{sec:lit-gc-hybrid-generational}

\Gls{generational garbage collection} is a type of \gls{hybrid
  collector}: the heap is divided into a number of
\textit{generations}, where each generation holds progressively older
\glspl{cell}. This is based on empirical observations of block
lifetimes resulting in the generational hypothesis: young objects die
quickly, whereas old objects stick around\cite{Ungar84}.

Allocation happens in the youngest generation, and when that fills up
a \textit{minor collection} is started. The generation is garbage
collected, and old enough blocks get copied to the next generation. In
the simplest case, all \glspl{live cell} at the time of a minor
collection get promoted. If the entire heap is full, a \textit{major
  collection} is started, which uses some other garbage collection
algorithm\cite{GarbageCollection}.

Generational garbage collection tends to perform well for languages
where \glspl{old-to-young pointer} are rare, such as most functional
languages, and so has become fairly popular amongst implementers of
such languages.

\section{Algorithm Verification}
\label{sec:lit-verification}

\Gls{algorithm verification} (or software verification) is the process
of constructing a mathematical proof that an algorithm behaves as we
expect it to. In the case of software, we may prove the actual code
implements the specification, in the case of an algorithm, we may be
satisfied with merely proving that it satisfies various properties
(such as termination).

We need verification because testing can never show that there are no
bugs, it can only show that there have been no bugs found yet. A
formal proof provides a much stronger guarantee than can be obtained
by testing or inspection alone.

There are two methods of verification: constructing the algorithm and
then proving things about it, and constructing a proof and then
extracting an algorithm out of it.

\subsection{Verification by Embedding}
\label{sec:lit-verification-embedding}

This is arguably the more difficult method of verification, as it
consists of taking a fairly low-level implementation (often at the
level of code) and a very high-level specification and proving things
about the two.

To begin with, the implementation must be formalised using some logic
such as Hoare logic\cite{Hoare69}, which define the semantics for a
very simple imperative language, or separation logic\cite{Reynolds02},
which is similar to Hoare logic but adds axioms for reasoning about
the stack and heap.

Once formalised, the axioms of the logic can be used to deduce
propositions about the implementation. This can be, to some extent,
automated using \glspl{proof assistant}, but in general this is
undecidable.

As an example of \gls{verification by embedding}, consider the
following short program, to find the index of the minimal element in a
sequence of integers:

\begin{verbatim}
i := 0;
j := 1;
while j < len xs do
    if xs[j] < xs[i] then
        i := j
    else
        skip
    j := j + 1
\end{verbatim}

We shall call this program $P$, and $xs$ is some sequence of
integers. For (partial) correctness of this program, we want to
establish (using Hoare triples) $\htriple{\len xs > 0}{P}{xs[i] = \min
  xs}$. A Hoare triple $\htriple{P}{S}{Q}$ states that if the
assertion $P$ holds before executing a statement $S$, then the
assertion $Q$ holds afterwards.

A full proof tree is displayed on page
\pageref{sec:lit-verification-embedding-example}, you may wish to
glance at that before reading the following sections, which explain
the ideas used in the proof, and then examine it more closely.

This approach is good to take if the property to be proven is more
easily expressed by code than by proof. For example, a garbage
collector may move things around in memory. Producing a constructive
proof (as is used in extraction) that this is soluble and then
plucking out the desired algorithm may be more difficult than
implementing the algorithm, and then showing that is satisfies a
``non-heap-clobbering'' invariant.

\subsubsection{Preconditions and Postconditions}
\label{sec:lit-verification-embedding-conditions}

A precondition is some assertion which must hold before executing some
code, and a postcondition is some assertion which must hold
afterwards. Hoare triples express this.

In the case of languages with formally specified semantics, we must be
able to derive the postcondition from the precondition and the
code. Sometimes ``backwards'' reasoning is used, by establishing the
desired postcondition (the result of executing the code), and then
working from end to beginning, in order to figure out what the
necessary preconditions are. If both the precondition and
postcondition of some sequence of code are known, then the proof can
progress inwards from both ends, meeting in the middle. If the two
proof efforts cannot be unified, then there is a flaw in the code.

Common to many proof systems is the ability to strengthen
preconditions and weaken postconditions. This is expressed by the
Hoare consequence rule:

\begin{prooftree}
  \AxiomC{$P' \implies P$}
  \AxiomC{$\htriple{P}{S}{Q}$}
  \AxiomC{$Q \implies Q'$}
  \TrinaryInfC{$\htriple{P'}{S}{Q'}$}
\end{prooftree}

This allows localised proofs to be used in a larger proof, and so
facilitates modularity.

\subsubsection{Axiomatic Semantics}
\label{sec:lit-verification-embedding-axioms}

Axiomatic semantics allow us to reason about programs by providing
tools to reason about the effect of program statements on program
state. These are closely connected with Hoare logic. A simple example
is the Hoare rule for composition:

\begin{prooftree}
  \AxiomC{$\htriple{P}{S}{Q}$}
  \AxiomC{$\htriple{Q}{T}{R}$}
  \BinaryInfC{$\htriple{P}{S; T}{R}$}
\end{prooftree}

This states that if we have two statements, $S$ and $T$, where the
postcondition of $S$ is the precondition of $T$, then we can combine
the two into the compound statement $S;T$\cite{Hoare69}. There are
rules for assignment, ``skip'' (the statement which does nothing),
while loops, conditional statements, and consequence.

In general, axiomatic semantics specify how the post-state of a
statement refers to the pre-state. They should be consistent, but (as
a consequence of G{\"o}del's incompleteness theorems), this cannot
necessarily be proven.

Given these rules, we can very easily deconstruct a program into a
proof tree showing how all of the assertions are related, which is
often a very good starting point for then constructing a specific
proof for the property we wish to establish.

\subsubsection{Loop Invariants}
\label{sec:lit-verification-embedding-invariants}

Proofs of programs involving loops rely on finding a \gls{loop
  invariant}: some property which does not change over time as the
loop is executed. This is expressed in Hoare logic with the while
rule:

\begin{prooftree}
  \AxiomC{$\htriple{I \land B}{S}{I}$}
  \UnaryInfC{$\htriple{I}{while B do S}{\lnot B \land I}$}
\end{prooftree}

$I$ is the loop invariant, $B$ is the loop condition, and $S$ is the
statement that is the loop body. The premise is read ``If the
proposition $I \land B$ holds, then after doing S once, the
proposition $I$ holds,'' and the consequence is read ``If the
proposition $I$ holds, then after doing the loop \textit{while B do
  S}, the proposition $\lnot B \land I$ holds''. Thus, we can see that
$I$ is invariant.

The only information we can ``get out'' of a loop, then (at least
under Hoare logic), is $\lnot B$, which doesn't seem very useful. The
loop may be doing something very complicated, but it seems we can only
extract a very small bit of information about this. The difficulty,
then, lies in finding a loop invariant that, when combined with $\lnot
B$, yields the result we want.

\begin{lscape}

\phantomsection
\label{sec:lit-verification-embedding-example}

\begin{prooftree}
  \AxiomC{$\htriple{I[1/j][0/i]}{i := 0}{I[0/j]}$}
  \AxiomC{$\htriple{I[1/j]}{j := 1}{I}$}
  \BinaryInfC{$\htriple{I[1/j][0/i]}{i := 0; j := 1}{I}$}
  \AxiomC{$I[1/j][0/i] \iff L$}
  \BinaryInfC{$\htriple{L}{i := 0; j := 1}{I}$}

  \AxiomC{$C_{T} \implies S[j/i]$}
  \UnaryInfC{$\htriple{C_{T}}{i := j}{S}$}

  \AxiomC{$C_{F} \iff S$}
  \AxiomC{$\htriple{C_{F}}{skip}{C_{F}}$}
  \BinaryInfC{$\htriple{C_{F}}{skip}{S}$}
  \BinaryInfC{$\htriple{I \land R(j)}{if \ldots}{S}$}

  \AxiomC{$S \implies I[j + 1/j]$}
  \AxiomC{$S \implies I$}
  \BinaryInfC{$\htriple{S}{j := j + 1}{I}$}

  \BinaryInfC{$\htriple{R(j) \land I}{if \ldots; j := j + 1}{I}$}
  \UnaryInfC{$\htriple{I}{while \ldots}{\lnot R(j) \land I}$}

  \BinaryInfC{$\htriple{L}{P}{C}$}
\end{prooftree}

\begin{multicols}{3}
\subsubsection{Common Definitions}
\begin{align*}
  L &\iff \len xs > 0\\
  R(x) &\iff x < \len xs\\
  C &\iff xs[i] = \min xs\\
  C_{T}&\iff I \land R(j) \land xs[j] < xs[i]\\
  C_{F}&\iff I \land R(j) \land \lnot(xs[j] < xs[i])\\
  I&\iff xs[i] = \min \{xs[a]~|~0 \leq a < j\}\\
  S&\iff I \land xs[i] \leq xs[j] \land R(j)
\end{align*}
\vfill

\subsubsection{Proofs}
\begin{align*}
  I[1/j][0/i] &\iff xs[0] = \min \{xs[a]~|~0 \leq a < 1\}\\
  &\iff \len xs > 0 \implies xs[0] = \min \{xs[0]\}\\
  &\iff \len xs > 0 \implies xs[0] = xs[0]\\
  &\iff \len xs > 0 \implies \true\\
  &\iff \len xs > 0\\
  \therefore I[1/j][0/i] &\iff L\\\\
%
  S[j/i] &\iff \left(I \land xs[i] \leq xs[j] \land R(j)\right)[j/i]\\
  &\iff I \land xs[j] \leq xs[j] \land R(j)\\
  &\iff I \land R(j)\\
  \therefore C_{T} &\implies S[j/i]\\\\
%
  I[j+1/j] &\iff I \land xs[i] \leq xs[j]\\
  \therefore S &\implies I[j+1/j]
\end{align*}

\begin{align*}
  \lnot\left(xs[j] < xs[i]\right) &\iff xs[i] \leq xs[j]\\
  \therefore C_{F} &\iff S\\\\
%
  I \land xs[i] \leq xs[j] \land R(j) &\implies I\\
  \therefore S &\implies I\\\\
%
  \lnot R(j) \land I &\iff j \geq \len xs \land xs[i] = \min
  \{xs[a]~|~0 \leq a < j\}\\
  &\iff xs[i] = \min \{x~|~x \in xs\}\\
  &\iff xs[i] = \min xs\\
  \therefore \lnot R(j) \land I &\implies C
\end{align*}
\vfill
\end{multicols}

\end{lscape}

\subsection{Verification by Extraction}
\label{sec:lit-verification-extraction}

The other approach is to gradually work downwards from the high-level
specification to the low-level implementation. This is called
\gls{verification by extraction}, or refinement.

We start out by formalising the specification in some logic, such as Z
or HOL, and then construct a slightly lower-level specification and
prove that it is a correct refinement of the higher-level one by
reasoning about the pre- and post-conditions of each abstract
operation. Whilst it may take many refinement steps to get actual code
out of this process, each proof is between two very similar
\glspl{specification}, and so may be easier than the verification by
proof method. \Glspl{proof assistant} may be more useful here than in
verification by proof, but here the difficulty lies in finding the
appropriate invariant to relate the two steps.

Furthermore, there are often multiple choices of refinement step
available, and so first few steps can be reused to construct different
algorithms, simply by making different choices at the lower levels.

Another method of extraction is to write a constructive proof that the
problem in question is soluble, and then to translate that into a
program. If the problem is simple enough to admit a constructive
proof, then this is a much simpler approach than refining all the way
down from a specification. However, for nontrivial problems,
constructing such a proof (easily) may not be possible.

A worked example is presented on page \pageref{lit_extraction}, and
some of the ideas used are discussed in the following sections.

This approach is good to take if the algorithm to be proven is quite
complex, and would be difficult to prove correct by itself, but where
the process of refining it from an initial specification may be
easier. For example, we could start out with a formalism for garbage
collection correctness, and then refine that until we produce an
actual collector (as done by Myreen\cite{Myreen10}).

\subsubsection{Constructive Proof}
\label{sec:lit-verification-extraction-constructive}

Often when proving that a problem is soluble, we do not care about how
an answer can be found, only that one exists. This can lead to very
concise existence proofs; when extracting a program directly from the
proof, however, we need to be able to show how to find the
answer. When refining from a specification, we start out with such an
abstract proof, and then gradually make it more constructive with each
step.

We can do this by using witnesses in our proof. A witness is a value
which shows a proposition to be correct. For example, $x = 2$ is a
witness for $\exists x \in \mathbb N, x > 1$. If we have witnesses for
everything we prove, then we know how to solve the problem.

\subsubsection{Case Analysis}
\label{sec:lit-verification-extraction-cases}

Proofs often diverge into multiple cases to be shown. For example, an
inductive proof (covered in more detail in the next section) has a
base case and an inductive case. Only by proving both cases do we
prove the whole.

Given a constructive proof, we can translate a case analysis to a
switch statement (or conditional statement) in our program, and use
the witnesses for each case to construct the solution to the entire
switch.

\subsubsection{Induction, Recursion, and Iteration}
\label{sec:lit-verification-extraction-induction}

Proofs by induction require proving a base case, and then proving an
inductive case given the inductive hypothesis (being able to solve a
slightly smaller problem) holds. This naturally translates to the
programming methodology of reduce-and-conquer.

We can translate induction into iteration as follows:

\begin{enumerate}
  \item The base case is set up by the program at the beginning
  \item The inductive case is performed one step at a time in a while
    loop
\end{enumerate}

This is similar to the ladder-climbing analogy for induction: if you
can get on the first rung, then you can get to the top, simply by
going up one rung at a time.

We can translate induction into recursion similarly:

\begin{enumerate}
  \item The base case is the recursive base case
  \item The inductive case is the recursive case
\end{enumerate}

This, perhaps, is a more natural translation of induction, as it
directly illustrates the splitting up of the problem into a slightly
smaller one.

\subsubsection{Types}
\label{sec:lit-verification-extraction-types}

We must be careful to honour any ``hidden'' assumptions in the
proof. For example, if a proof requires a value to be a natural
number, then we cannot use machine words in our program, unless it can
also be proven that the natural numbers in question will always fit
into this range.

More generally, types in programming languages may not be identical to
types used in proofs, and we must be careful not to invalidate the
proof by getting the implementation types incorrect.

\subsubsection{Program Transformations}
\label{sec:lit-verification-extraction-transformation}

When we extract a program from a proof, the structure of the program
very closely mirrors that of the proof, as would be expected. However,
this may not be considered good style. We can apply arbitrary
transformations to the code of the extracted program, as long as these
transformations preserve the semantics.

\subsubsection{Worked Example}
\label{sec:lit-verification-extraction-example}

\phantomsection
\label{lit_extraction}

\begin{theorem}[Minimum Index]
  For all nonempty sequences of integers, there exists an index of a
  minimum element.

  \[\forall n \in \mathbb N,\ 
  \forall x_{1}, x_{2}, \ldots x_{n} \in \mathbb Z,\ 
  \exists 1 \leq m \leq n,\ 
  \forall 1 \leq i \leq n,\ 
  x_{m} \leq x_{i}\]
\end{theorem}

\begin{proof}
  By induction.

  \begin{description}
    \item[Base case (n=1)] $m = 1$ is a witness, by reflexivity of
      $\leq$.

    \item[Inductive case (n=k+1)] Divide the sequence into its head
      and tail (the latter being of length $k$). Call the head $x$ and
      the tail $xs$, for simplicity. Solve the case for $xs$, and call
      the witness $m_{k}$.
      \begin{description}
        \item[If $x \leq xs_{m_k}$] Then $m = 1$ is a witness, by
          transitivity of $\leq$.

        \item[If $xs_{m_k} < x$] Then $m = m_{k} + 1$ is a witness, as
          $xs_{1}$ is actually as position 2 in the original sequence
          (it is all shifted right by one place).
      \end{description}
  \end{description}
\end{proof}

We can now, fairly directly, translate this proof into the following
Haskell program:

\begin{lstlisting}[language=haskell]
import Data.List (genericIndex)

f :: [Integer] -> Integer
f sequence = case sequence of
               [_] -> 1
               (x:xs) -> let mk = f xs
                        in if x <= genericIndex xs (mk - 1)
                           then 1
                           else 1 + mk
\end{lstlisting}

Even in such a short program, there are a couple of language-imposed
complications, due to the mismatch of types. Firstly, the
\texttt{genericIndex} function has to be used, as the list-indexing
operator \texttt{(!!)} takes the index as a machine word; and
secondly, the \texttt{mk} value has to be decremented when indexing,
as list indices in Haskell start from 0, not 1.

We can now apply semantics-preserving transformations to produce more
idiomatic code:

\begin{lstlisting}[language=haskell]
f :: [Integer] -> Integer
f (x:xs) | x <= genericIndex xs (mk - 1) = 1
         | otherwise = 1 + mk
         where mk = f xs
f _ = 1
\end{lstlisting}

\section{Verified Garbage Collection}
\label{sec:lit-verifiedgc}

Finally, we look at two examples of \glspl{garbage collector} which
have been verified in different ways.

\subsection{Reusable verification of a copying collector}
\label{sec:lit-verifiedgc-myreen}

Myreen\cite{Myreen10} has constructed a verified \gls{garbage
  collector} using the extraction method: correctness was formalised
using the HOL4 theorem prover, was refined to produce an algorithm for
correct \gls{copying} \glspl{collector}, and was then refined further
to the level of verified machine code.

The \glspl{collector} produced have been named L1, which is a
specification of copying \gls{garbage collection}; L2, which is an
implementation of L1 as the transitive closure of a step relation; L3,
which is a deterministic implementation of L2 with more realistic
memory semantics; L4, which uses actual implementation types; and L5,
which is verified machine code. L5 is constructed by earlier work of
Myreen, on constructing a certifying compiler.

Other than the obvious, the main contribution of Myreen is that his
proof is quite generic, as only the lowest level of refinement (L5)
makes use of a specific programming logic, and could be applied to many
different copying collectors, just by changing the specific refinement
steps taken.

As is shown in chapter 4, his L1 collector can be generalised to
non-copying collectors, with minor changes to the formalism.

\subsection{Local Reasoning about a Copying Garbage Collector}
\label{sec:lit-verifiedgc-birkedal}

Birkedal, Torp-Smith, and Reynolds\cite{Birkedal04} take a very
different approach to Myreen, they take an existing \gls{garbage
  collector} (Cheney's copying collector) and prove it correct, using
a variant of separation logic.

Similarly to Myreen, they define correctness in terms of a heap
isomorphism modulo garbage cells, and in particular, that the
resultant heap has no garbage cells whatsoever.

The motivation of this paper is to be a stepping stone towards
programming logics which incorporate \gls{garbage collection}, and so
being able to prove the correctness of the combination of a program
and a language runtime, which has thus far not been feasible.

This paper contributes to the field an extension of separation logic
with semantics for more types of assertion over finite sets, which is
used to express the isomorphism property of the algorithm. Needless to
say, this is an essential property of any garbage collector which
moves data around in the heap.

The major difference between these two verified garbage collectors is
that Myreen used extraction whereas Birkedal \textit{et al.} used
proof. Thus, this proof cannot really be generalised to other
collectors (although the definition of correctness and logic developed
can), whereas Myreen's proofs are much more applicable to copying
collectors in general.
